

------------- initial steps --------------
        1. Redis Cluster requires manual cluster creation:
        ```
        docker exec -it redis-node-1 redis-cli --cluster create  redis-node-1:7000 redis-node-2:7001 redis-node-3:7002  --cluster-replicas 0
        test it :
            docker exec -it redis-node-1 sh
                redis-cli -c -p 7000
                    cluster nodes
                        SET test-key "hello"
                        GET test-key

        ```
        2.db
                -- 1. Cumulative frequency table
                CREATE TABLE IF NOT EXISTS prefix_query_frequency (
                  prefix VARCHAR(60)    NOT NULL,
                  query  VARCHAR(500)   NOT NULL,
                  frequency INT         NOT NULL,
                  last_updated TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                  PRIMARY KEY (prefix, query)
                ) ENGINE=InnoDB;

                -- 2. Top-K suggestions table
                CREATE TABLE IF NOT EXISTS prefix_suggestions (
                  prefix       VARCHAR(60) NOT NULL,
                  completions  JSON        NOT NULL,
                  last_updated TIMESTAMP   NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                  PRIMARY KEY (prefix)
                ) ENGINE=InnoDB;

        username:root password :root

        insert a raw for test debezium and kafka

        INSERT INTO `autocomplete`.`prefix_suggestions`
        (`prefix`,
        `completions`,
        `last_updated`)
        VALUES
        ('fudv',
        '["fudv hhhhhahaha"]',
        '2025-05-30 22:08:15');



        3. after testing the spark and its successfully write in the mongo . you should create the file my.cnf and add
        it to the volume in the mysql container . then enable the debezium run this in the windows (host):
        use postman:

        "
                curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d

                {
                  "name": "mysql-autocomplete-connector",
                  "config": {
                    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
                    "database.hostname": "mysql",
                    "database.port": "3306",
                    "database.user": "root",
                    "database.password": "root",
                    "database.server.id": "223344",
                    "database.server.name": "autocomplete",
                    "database.include.list": "autocomplete",
                    "table.include.list": "autocomplete.prefix_suggestions",
                    "include.schema.changes": "false",
                    "schema.history.internal.kafka.bootstrap.servers": "kafka:9092",
                    "schema.history.internal.kafka.topic": "schema-changes.autocomplete",
                    "topic.prefix": "autocomplete"
                  }
                }



            "


            This curl command sends a POST request to the Debezium Connect REST API (http://localhost:8083/connectors) to create a new MySQL source connector. This connector will:
            - Monitor changes in the autocomplete.prefix_suggestions table inside the autocomplete database.
            - Stream those changes to a Kafka topic named autocomplete.autocomplete.prefix_suggestions.
            - Use the MySQL binary logs (binlog) you configured earlier to track inserts, updates, and deletes.

            Note : in the debezium log dont care about this error:
            "2025-05-31 23:02:27,935 INFO || [Worker clientId=connect-1, groupId=debezium] Node -1 disconnected. [org.apache.kafka.clients.NetworkClient]"

            4. kafka

            no need to set up anything just test after sent the request to the debezium with :
                 docker exec -it kafka bash
                [appuser@3103e4c3d49c ~]$ kafka-topics --bootstrap-server kafka:9092 --list
                reesult :
                        __consumer_offsets
                        autocomplete.autocomplete.prefix_suggestions
                        my_connect_configs
                        my_connect_offsets
                        my_connect_statuses
                        schema-changes.autocomplete
                   consum it :
                    kafka-console-consumer --bootstrap-server kafka:9092  --topic autocomplete.autocomplete.prefix_suggestions   --from-beginning





------------------------------------------------------

------------- steps explanation-----------
1. File Structure in HDFS
    Each hour, a new file is added. Spark reads only the last 24 hourly files
    /logs/2025-05-28-01.txt
    /logs/2025-05-28-02.txt
    ...
    /logs/2025-05-28-23.txt

2. Spark job will:
   List all HDFS files for the last 24 hours (using a naming convention).
   Read and combine them into a single RDD/DataFrame.
   Track frequencies cumulatively (same as now).
   Output top-K per prefix.
   Save to the temp SQL DB (instead of Kafka).

3. Spark Logic Recap
   Each hour:
   Load last hour’s file from HDFS (e.g., /logs/2025-05-28-12.txt)
   Extract (prefix, query) pairs
   Update/increment prefix_query_frequency table
   For each prefix, get top K queries by frequency
   Format them as JSON
   Overwrite or upsert them into prefix_suggestions

4. Temp SQL DB Output
   Instead of Kafka, use JDBC(java database connectivity) to save directly to MySQL .
   We will store two tables in the MySQL temp database:
     prefix_query_frequency (Intermediate Table)
        This table stores cumulative frequencies for each (prefix, query) pair
        It remembers how many times a query appeared under each prefix.
        Example content:
        prefix	query	frequency	last_updated
        "he"	"hello world"	42	2025-05-28 12:00:00
     prefix_suggestions (Final Temp Output)
        This table stores the top-K suggestions per prefix (used for syncing to Redis via Debezium → Kafka).
        Example content:
        prefix                          	completions	                                      last_updated
        "he"	[{"query":"hello world","frequency":42},{"query":"hey","frequency":18}]	   2025-05-28 12:00:00


5.


---------------------------- run -------------------
------------ docker -------------------
-----------------------------------

build this project with
mvn clean package
go to the db and creat the tabels see section above
docker compose down --volumes
docker compose build --no-cache
 docker volume rm $(docker volume ls -q)
docker-compose --profile part1 --profile part2 up
docker-compose --profile part2 up
docker compose up -d

docker-compose rm -sfv policy-service
    This will:

        -s: stop the container

        -f: force remove it

        -v: remove any associated anonymous or named volumes
docker-compose up -d --build policy-service
-----------------------------------
--- wait 29 second first
powershell -ExecutionPolicy Bypass -File upload_to_hdfs.ps1
if you got error with policy run this
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass


then verify
docker exec -it namenode hdfs dfs -ls /logs
check if the build contin the connecteor
jar tf /opt/spark-apps/your-job.jar | grep mysql

docker exec -it spark-master  /spark/bin/spark-submit   --master spark://spark-master:7077  --class me.spark.IncrementalAutocomplete  --jars /opt/spark/jars/mysql-connector-j-8.0.33.jar    /opt/spark-apps/spark-hdfs.jar   hdfs://namenode:8020/logs   2025-05-30-15   jdbc:mysql://mysql:3306/autocomplete   prefix_query_frequency   prefix_suggestions   10

or first run
 docker exec -it spark-master /bin/bash

test the second part (from mysql >>>)


check the redis
docker exec -it redis-node-1 redis-cli -c -p 7000
get kheded74r
